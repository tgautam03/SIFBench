{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "cb02c4e5",
   "metadata": {},
   "source": [
    "# Dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "eec4eed7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "from concurrent.futures import ThreadPoolExecutor\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from scipy.interpolate import PchipInterpolator\n",
    "\n",
    "from tqdm import tqdm\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "99fc6380",
   "metadata": {},
   "outputs": [],
   "source": [
    "def drop_rows_with_nan(df, how='any', subset=None):\n",
    "    \"\"\"\n",
    "    Removes rows containing NaN values from a Pandas DataFrame.\n",
    "\n",
    "    Args:\n",
    "        df: The Pandas DataFrame to process.\n",
    "        how: 'any' to drop rows containing *any* NaN values, 'all' to drop only rows where *all* values are NaN.\n",
    "        subset: An optional list of column names to consider. If None, all columns are checked.\n",
    "\n",
    "    Returns:\n",
    "        pandas.DataFrame: A new DataFrame with the NaN-containing rows removed.\n",
    "                          The original DataFrame is not modified.\n",
    "    \"\"\"\n",
    "    # Create a copy to avoid modifying the original DataFrame\n",
    "    df_cleaned = df.copy()\n",
    "    df_cleaned = df_cleaned.dropna(axis=0, how=how, subset=subset)\n",
    "    return df_cleaned"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c5f8379",
   "metadata": {},
   "source": [
    "# Working on..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e9630a28",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['FA7000-15-D-0006-19FA202', 'BH_SINGLE', 'FA700021D0002-23F0143', 'FA7000-15-D-0006-0016', 'FA7000-15-D-0006-9011', 'FA700021D0002-22F0214', 'TWIN', 'FA7000-15-D-0006-20F0212']\n",
      "['BH']\n"
     ]
    }
   ],
   "source": [
    "# Specify the directory path\n",
    "dir_path = \"../files/data/RAW\"\n",
    "\n",
    "# Get folder names\n",
    "master_folders = [f for f in os.listdir(dir_path) if os.path.isdir(os.path.join(dir_path, f))]\n",
    "\n",
    "print(master_folders)\n",
    "\n",
    "# Specify the directory path\n",
    "dir_path = \"../files/data/RAW/{}\".format(master_folders[6])\n",
    "\n",
    "# Get folder names\n",
    "folders = [f for f in os.listdir(dir_path) if os.path.isdir(os.path.join(dir_path, f))]\n",
    "\n",
    "print(folders)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33532659",
   "metadata": {},
   "source": [
    "# .DAT to .csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "5fd349b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_file(file_info):\n",
    "    input_file_path, output_file_path = file_info\n",
    "    \n",
    "    rows = []\n",
    "    with open(input_file_path, 'r') as file:\n",
    "        lines = file.readlines()\n",
    "\n",
    "        W_R, a_c, a_t, r_t = None, None, None, None\n",
    "        c_index = 0\n",
    "\n",
    "        for i, line in enumerate(lines):\n",
    "            values = line.strip().split()\n",
    "            if not values:\n",
    "                continue\n",
    "\n",
    "            if values[0] == \"Geometry:\":\n",
    "                # Extract the constant multiplying thickness\n",
    "                constant_match = re.search(r'Width=Height=(\\d+)\\*thickness', line)\n",
    "                constant_value = int(constant_match.group(1)) if constant_match else None\n",
    "                # print(\"Const: \", constant_value)\n",
    "\n",
    "                # Extract the thickness value\n",
    "                thickness_match = re.search(r'Thickness=\\s*([\\d.]+)', line)\n",
    "                thickness_value = float(thickness_match.group(1)) if thickness_match else None\n",
    "                # print(\"Thickness: \", thickness_value)\n",
    "\n",
    "                # Extract the radius value\n",
    "                radius_match = re.search(r'Hole radius:\\s*([\\d.]+)', line)\n",
    "                radius_value = float(radius_match.group(1)) if radius_match else None\n",
    "                # print(\"Radius: \", radius_value)\n",
    "\n",
    "                W = constant_value*thickness_value\n",
    "                W_R = W / radius_value\n",
    "\n",
    "            elif values[0] == \"Scenario:\":\n",
    "                c_index = values[1]\n",
    "\n",
    "            elif values[0] == \"ndom\" and (values[1] == \"a1/c1\" or values[1] == \"a2/c2\"):\n",
    "                crack = False\n",
    "                next_values = lines[i + 1].strip().split()\n",
    "                a_c, a_t, r_t = map(float, next_values[1:4])\n",
    "\n",
    "            elif values[0] == \"crack\":\n",
    "                crack = True\n",
    "\n",
    "            elif values[0].isdigit() and crack:\n",
    "                row = [c_index] + list(map(float, values[0])) + [W_R, a_c, a_t, r_t] + list(map(float, values[5:9]))\n",
    "                rows.append(row)\n",
    "\n",
    "    df = pd.DataFrame(rows, columns=['c_index', 'crack', 'W/R', 'a/c', 'a/t', 'r/t', 'phi', 'K-T', 'K-B', 'K-P'])\n",
    "    df.to_csv(output_file_path, index=False)\n",
    "\n",
    "master_folder = master_folders[6]\n",
    "tasks = []\n",
    "\n",
    "for folder in folders:\n",
    "    dir_path = os.path.join(\"../files/data/RAW\", master_folder, folder)\n",
    "    files = [f for f in os.listdir(dir_path) if os.path.isfile(os.path.join(dir_path, f))]\n",
    "\n",
    "    for file_name in files:\n",
    "        input_file_path = os.path.join(dir_path, file_name)\n",
    "        output_file_path = os.path.join(dir_path, f\"{file_name[:-6]}.csv\")\n",
    "        tasks.append((input_file_path, output_file_path))\n",
    "\n",
    "for i in range(len(tasks)):\n",
    "    process_file(tasks[i])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c28ea827",
   "metadata": {},
   "source": [
    "# Cleaning CSV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "d8f109fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assume drop_rows_with_nan is defined elsewhere and works correctly\n",
    "# Example placeholder:\n",
    "def drop_rows_with_nan(df):\n",
    "    \"\"\"Drops rows with any NaN values.\"\"\"\n",
    "    return df.dropna().copy() # Added .copy() to avoid SettingWithCopyWarning\n",
    "\n",
    "\n",
    "# --- Function to process a single c_index group ---\n",
    "# This function will handle the logic for one unique c_index\n",
    "def process_c_index_group(group_df):\n",
    "    \"\"\"\n",
    "    Processes a single DataFrame group corresponding to a unique c_index.\n",
    "    Performs crack splitting, parameter extraction, interpolation,\n",
    "    and constructs the 128x16 output block.\n",
    "\n",
    "    Returns the 128x16 numpy array block or None if processing fails.\n",
    "    \"\"\"\n",
    "    c_index = group_df['c_index'].iloc[0] # Get the c_index value for this group\n",
    "\n",
    "    # Split the group by crack type (assuming 'crack' is the column name)\n",
    "    try:\n",
    "        crack1_df = group_df[group_df['crack'] == 1]\n",
    "        crack2_df = group_df[group_df['crack'] == 2]\n",
    "\n",
    "        if crack1_df.empty or crack2_df.empty:\n",
    "             # This case should ideally be caught by the initial corrupt_indices check,\n",
    "             # but as a safeguard within the group processing.\n",
    "            print(f\"Warning: c_index {c_index} does not have data for both cracks 1 and 2 after grouping.\")\n",
    "            return None\n",
    "\n",
    "    except KeyError:\n",
    "        print(f\"Error: 'crack' column not found for c_index {c_index}.\")\n",
    "        return None\n",
    "    except Exception as e:\n",
    "        print(f\"Error splitting crack data for c_index {c_index}: {e}\")\n",
    "        return None\n",
    "\n",
    "\n",
    "    # --- Extract Unique Parameters ---\n",
    "    # Assuming these parameters are constant for a given c_index\n",
    "    # and match the column names implied by your original index access\n",
    "    try:\n",
    "        # Use .iloc[0] to get the first value, assuming they are constant within the group\n",
    "        wr = crack1_df['W/R'].iloc[0] # Original: data[:,1]\n",
    "        rt = crack1_df['r/t'].iloc[0] # Original: data[:,6]\n",
    "        bt = 1 # Original: data[:,7]\n",
    "\n",
    "        # These are crack-specific parameters\n",
    "        a1c1 = crack1_df['a/c'].iloc[0] # Original: data[:,2] (crack 1)\n",
    "        a1t = crack1_df['a/t'].iloc[0]  # Original: data[:,3] (crack 1)\n",
    "\n",
    "        a2c2 = crack2_df['a/c'].iloc[0] # Original: data[:,4] (crack 2)\n",
    "        a2t = crack2_df['a/t'].iloc[0]  # Original: data[:,5] (crack 2)\n",
    "\n",
    "    except (KeyError, IndexError) as e:\n",
    "         print(f\"Error extracting parameters for c_index {c_index}: {e}\")\n",
    "         return None\n",
    "    except Exception as e:\n",
    "        print(f\"Error extracting parameters for c_index {c_index}: {e}\")\n",
    "        return None\n",
    "\n",
    "    # --- Prepare for Interpolation ---\n",
    "    # Extract relevant columns and convert to numpy *once* per crack group\n",
    "    try:\n",
    "        # Assuming 'phi', 'K-T', 'K-B', 'K-P' are the last 4 columns as per original indexing logic\n",
    "        crack1_np = crack1_df[['phi', 'K-T', 'K-B', 'K-P']].to_numpy()\n",
    "        crack2_np = crack2_df[['phi', 'K-T', 'K-B', 'K-P']].to_numpy()\n",
    "    except KeyError:\n",
    "        print(f\"Error: Missing expected K or phi columns for c_index {c_index}.\")\n",
    "        return None\n",
    "    except Exception as e:\n",
    "        print(f\"Error converting crack data to numpy for c_index {c_index}: {e}\")\n",
    "        return None\n",
    "\n",
    "\n",
    "    # --- Interpolation Logic ---\n",
    "\n",
    "    # Determine the target phi grid (same for both cracks per c_index)\n",
    "    # Based on the original code's logic taking min/max phi from each crack data\n",
    "    try:\n",
    "        phi_vals1 = crack1_np[:, 0]\n",
    "        phi_vals2 = crack2_np[:, 0]\n",
    "\n",
    "        phi_min_combined = min(phi_vals1.min(), phi_vals2.min()) + 0.035\n",
    "        phi_max_combined = max(phi_vals1.max(), phi_vals2.max()) - 0.035\n",
    "\n",
    "        # Ensure valid range for linspace\n",
    "        if phi_min_combined >= phi_max_combined:\n",
    "            print(f\"Warning: Calculated phi_min >= phi_max for c_index {c_index}. Skipping interpolation.\")\n",
    "            return None\n",
    "\n",
    "        # Generate the target phi grid (128 points as per original code structure)\n",
    "        phi_grid_target = np.linspace(phi_min_combined, phi_max_combined, 132)[2:-2]\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Error calculating phi grid for c_index {c_index}: {e}\")\n",
    "        return None\n",
    "\n",
    "    # Helper function for interpolation (reduces code repetition)\n",
    "    def interpolate_crack_data(crack_np, phi_grid_target, phi_min_combined, phi_max_combined):\n",
    "        phi_vals = crack_np[:, 0]\n",
    "        k_t_vals = crack_np[:, 1]\n",
    "        k_b_vals = crack_np[:, 2]\n",
    "        k_p_vals = crack_np[:, 3]\n",
    "\n",
    "        # Apply phi range filtering using the combined min/max + offset\n",
    "        filtered_indices = (phi_vals >= phi_min_combined) & (phi_vals <= phi_max_combined)\n",
    "        phi_vals_filtered = phi_vals[filtered_indices]\n",
    "        k_t_vals_filtered = k_t_vals[filtered_indices]\n",
    "        k_b_vals_filtered = k_b_vals[filtered_indices]\n",
    "        k_p_vals_filtered = k_p_vals[filtered_indices]\n",
    "\n",
    "\n",
    "        # Need at least 2 data points for Pchip interpolation\n",
    "        if len(phi_vals_filtered) < 2:\n",
    "             print(f\"Warning: Not enough valid data points ({len(phi_vals_filtered)}) for interpolation.\")\n",
    "             return None, None, None\n",
    "\n",
    "        # Sort phi values for monotonic input required by PchipInterpolator\n",
    "        sort_indices = np.argsort(phi_vals_filtered)\n",
    "        phi_vals_sorted = phi_vals_filtered[sort_indices]\n",
    "\n",
    "        # Create monotonic indices (in case of identical phi values after filtering)\n",
    "        monotonic_indices = [0]\n",
    "        for i in range(1, len(phi_vals_sorted)):\n",
    "             if phi_vals_sorted[i] > phi_vals_sorted[monotonic_indices[-1]]:\n",
    "                 monotonic_indices.append(i)\n",
    "\n",
    "        if len(monotonic_indices) < 2:\n",
    "             print(f\"Warning: Not enough monotonic phi points ({len(monotonic_indices)}) for interpolation.\")\n",
    "             return None, None, None\n",
    "\n",
    "        # Perform Pchip interpolation for each K value\n",
    "        try:\n",
    "            interp_kt = PchipInterpolator(phi_vals_sorted[monotonic_indices], k_t_vals_filtered[sort_indices][monotonic_indices], extrapolate=False)\n",
    "            kt_interp = interp_kt(phi_grid_target)\n",
    "\n",
    "            interp_kb = PchipInterpolator(phi_vals_sorted[monotonic_indices], k_b_vals_filtered[sort_indices][monotonic_indices], extrapolate=False)\n",
    "            kb_interp = interp_kb(phi_grid_target)\n",
    "\n",
    "            interp_kp = PchipInterpolator(phi_vals_sorted[monotonic_indices], k_p_vals_filtered[sort_indices][monotonic_indices], extrapolate=False)\n",
    "            kp_interp = interp_kp(phi_grid_target)\n",
    "\n",
    "            return kt_interp, kb_interp, kp_interp\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"Error during Pchip interpolation: {e}\")\n",
    "            return None, None, None\n",
    "\n",
    "\n",
    "    # Perform interpolation for both cracks\n",
    "    kt1_interp, kb1_interp, kp1_interp = interpolate_crack_data(crack1_np, phi_grid_target, phi_min_combined, phi_max_combined)\n",
    "    kt2_interp, kb2_interp, kp2_interp = interpolate_crack_data(crack2_np, phi_grid_target, phi_min_combined, phi_max_combined)\n",
    "\n",
    "    # Check if interpolation was successful for both cracks\n",
    "    if any(k is None for k in [kt1_interp, kb1_interp, kp1_interp, kt2_interp, kb2_interp, kp2_interp]):\n",
    "        print(f\"Interpolation failed for c_index {c_index}. Skipping.\")\n",
    "        return None\n",
    "\n",
    "\n",
    "    # --- Construct the 128x16 Output Block ---\n",
    "    # Pre-allocate the block for this c_index\n",
    "    data_block = np.zeros((128, 16))\n",
    "\n",
    "    # Populate the block based on the desired column structure\n",
    "    data_block[:, 0] = c_index\n",
    "    data_block[:, 1] = wr\n",
    "    data_block[:, 2] = a1c1\n",
    "    data_block[:, 3] = a1t\n",
    "    data_block[:, 4] = a2c2 # a2/c2 from crack 2\n",
    "    data_block[:, 5] = a2t  # a2/t from crack 2\n",
    "    data_block[:, 6] = rt\n",
    "    data_block[:, 7] = bt\n",
    "    data_block[:, 8] = phi_grid_target # phi_1 is the target grid\n",
    "    data_block[:, 9] = phi_grid_target # phi_2 is the target grid\n",
    "    data_block[:, 10] = kt1_interp\n",
    "    data_block[:, 11] = kt2_interp\n",
    "    data_block[:, 12] = kb1_interp\n",
    "    data_block[:, 13] = kb2_interp\n",
    "    data_block[:, 14] = kp1_interp\n",
    "    data_block[:, 15] = kp2_interp\n",
    "\n",
    "    # Final check for NaNs resulting from interpolation outside valid range (extrapolate=False)\n",
    "    if np.isnan(data_block).any():\n",
    "        # print(f\"Warning: NaN values generated in final data block for c_index {c_index}. Skipping.\")\n",
    "        return None\n",
    "\n",
    "    return data_block"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "6666e698",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Working on: CASE24-TWIN-Rt-1dot0.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing CASE24-TWIN-Rt-1dot0.csv: 100%|██████████| 37636/37636 [01:39<00:00, 378.27group/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully saved cleaned data for CASE24-TWIN-Rt-1dot0.csv to ../files/data/TWIN/BH/CLEANED/CASE24-TWIN-Rt-1dot0-CLEANED.csv\n",
      "Working on: CASE24-TWIN-Rt-0dot5.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing CASE24-TWIN-Rt-0dot5.csv: 100%|██████████| 37636/37636 [01:36<00:00, 388.71group/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully saved cleaned data for CASE24-TWIN-Rt-0dot5.csv to ../files/data/TWIN/BH/CLEANED/CASE24-TWIN-Rt-0dot5-CLEANED.csv\n",
      "Working on: CASE24-TWIN-Rt-0dot2.csv\n",
      "Dropped 157528 rows with NaN in CASE24-TWIN-Rt-0dot2.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing CASE24-TWIN-Rt-0dot2.csv: 100%|██████████| 36864/36864 [01:33<00:00, 394.99group/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully saved cleaned data for CASE24-TWIN-Rt-0dot2.csv to ../files/data/TWIN/BH/CLEANED/CASE24-TWIN-Rt-0dot2-CLEANED.csv\n",
      "Working on: CASE24-TWIN-Rt-3dot0.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing CASE24-TWIN-Rt-3dot0.csv: 100%|██████████| 36864/36864 [01:37<00:00, 377.16group/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully saved cleaned data for CASE24-TWIN-Rt-3dot0.csv to ../files/data/TWIN/BH/CLEANED/CASE24-TWIN-Rt-3dot0-CLEANED.csv\n",
      "Working on: CASE24-TWIN-Rt-2dot0.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing CASE24-TWIN-Rt-2dot0.csv: 100%|██████████| 37636/37636 [01:37<00:00, 386.58group/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully saved cleaned data for CASE24-TWIN-Rt-2dot0.csv to ../files/data/TWIN/BH/CLEANED/CASE24-TWIN-Rt-2dot0-CLEANED.csv\n",
      "Processing complete.\n"
     ]
    }
   ],
   "source": [
    "# --- Main Processing Loop for Files ---\n",
    "\n",
    "dir_path = \"../files/data/TWIN/BH\"\n",
    "# Create the output directory if it doesn't exist\n",
    "output_dir = os.path.join(dir_path, \"CLEANED\")\n",
    "os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "\n",
    "csv_files = [f for f in os.listdir(dir_path) if f.endswith(\".csv\")]\n",
    "\n",
    "\n",
    "for csv_index in csv_files: # Continue processing from file 70\n",
    "    print(f\"Working on: {csv_index}\")\n",
    "    file_path = os.path.join(dir_path, csv_index)\n",
    "    output_path = os.path.join(output_dir, f\"{csv_index[:-4]}-CLEANED.csv\")\n",
    "\n",
    "    # Skip if the cleaned file already exists (optional, for resuming)\n",
    "    if os.path.exists(output_path):\n",
    "        print(f\"Cleaned file {output_path} already exists. Skipping.\")\n",
    "        continue\n",
    "\n",
    "    try:\n",
    "        # Read the CSV file\n",
    "        df = pd.read_csv(file_path)\n",
    "\n",
    "        # Drop rows with NaN values (assuming drop_rows_with_nan is efficient)\n",
    "        initial_rows = len(df)\n",
    "        df = drop_rows_with_nan(df)\n",
    "        if len(df) < initial_rows:\n",
    "            print(f\"Dropped {initial_rows - len(df)} rows with NaN in {csv_index}\")\n",
    "\n",
    "        if df.empty:\n",
    "             print(f\"No data left in {csv_index} after dropping NaNs. Skipping.\")\n",
    "             continue\n",
    "\n",
    "        # Check for corrupt indices where 'crack' count is not 2 per 'c_index'\n",
    "        # Assuming 'crack' is the correct column name here based on your code\n",
    "        if 'c_index' not in df.columns or 'crack' not in df.columns:\n",
    "             print(f\"Error: Missing 'c_index' or 'crack' column in {csv_index}. Skipping.\")\n",
    "             continue\n",
    "\n",
    "        unique_counts = df.groupby('c_index')['crack'].nunique()\n",
    "        corrupt_indices = unique_counts[unique_counts != 2].index\n",
    "\n",
    "        # Filter out rows belonging to corrupt c_index values\n",
    "        df = df[~df['c_index'].isin(corrupt_indices)].copy() # Use .copy() to avoid SettingWithCopyWarning\n",
    "\n",
    "        unique_index_list = df['c_index'].unique().tolist()\n",
    "        num_valid_indices = len(unique_index_list)\n",
    "\n",
    "        if num_valid_indices == 0:\n",
    "             print(f\"No valid c_index entries left in {csv_index} after cleaning. Skipping.\")\n",
    "             continue\n",
    "\n",
    "        # --- Process Groups and Collect Results ---\n",
    "        processed_blocks = []\n",
    "\n",
    "        # Group by c_index and iterate through groups\n",
    "        grouped = df.groupby('c_index')\n",
    "\n",
    "        # Use tqdm to show progress over the groups within the file\n",
    "        for c_index, group_df in tqdm(grouped, desc=f\"Processing {csv_index}\", unit=\"group\"):\n",
    "            # Process each group using the dedicated function\n",
    "            data_block = process_c_index_group(group_df) # Pass the group DataFrame\n",
    "\n",
    "            if data_block is not None:\n",
    "                processed_blocks.append(data_block)\n",
    "\n",
    "        # --- Combine and Save ---\n",
    "        if not processed_blocks:\n",
    "             print(f\"No valid processed blocks generated for {csv_index}. Skipping save.\")\n",
    "             continue\n",
    "\n",
    "        # Concatenate all collected blocks into the final array\n",
    "        d_final = np.concatenate(processed_blocks, axis=0)\n",
    "\n",
    "        # Convert the final numpy array to a DataFrame\n",
    "        # Ensure column names match the structure of d_final\n",
    "        output_df = pd.DataFrame(d_final, columns=[\n",
    "            \"c_index\", \"W/R\", \"a1/c1\", \"a1/t\", \"a2/c2\", \"a2/t\",\n",
    "            \"r/t\", \"b/t\", \"phi_1\", \"phi_2\", \"K1-T\", \"K2-T\",\n",
    "            \"K1-B\", \"K2-B\", \"K1-P\", \"K2-P\"\n",
    "        ])\n",
    "\n",
    "        # Save the cleaned and processed DataFrame to a new CSV file\n",
    "        output_df.to_csv(output_path, index=False) # index=False prevents writing the DataFrame index\n",
    "        print(f\"Successfully saved cleaned data for {csv_index} to {output_path}\")\n",
    "\n",
    "    except FileNotFoundError:\n",
    "        print(f\"Error: File not found at {file_path}. Skipping.\")\n",
    "        continue\n",
    "    except Exception as e:\n",
    "        print(f\"An unexpected error occurred while processing {csv_index}: {e}\")\n",
    "        # Optional: print traceback for debugging\n",
    "        # import traceback\n",
    "        # traceback.print_exc()\n",
    "        continue # Continue to the next file\n",
    "\n",
    "\n",
    "print(\"Processing complete.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75ae70e7",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
