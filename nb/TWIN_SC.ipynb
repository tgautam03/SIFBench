{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "cb02c4e5",
   "metadata": {},
   "source": [
    "# Dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "eec4eed7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "import re\n",
    "from concurrent.futures import ThreadPoolExecutor\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from scipy.interpolate import PchipInterpolator\n",
    "\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "99fc6380",
   "metadata": {},
   "outputs": [],
   "source": [
    "def drop_rows_with_nan(df, how='any', subset=None):\n",
    "    \"\"\"\n",
    "    Removes rows containing NaN values from a Pandas DataFrame.\n",
    "\n",
    "    Args:\n",
    "        df: The Pandas DataFrame to process.\n",
    "        how: 'any' to drop rows containing *any* NaN values, 'all' to drop only rows where *all* values are NaN.\n",
    "        subset: An optional list of column names to consider. If None, all columns are checked.\n",
    "\n",
    "    Returns:\n",
    "        pandas.DataFrame: A new DataFrame with the NaN-containing rows removed.\n",
    "                          The original DataFrame is not modified.\n",
    "    \"\"\"\n",
    "    # Create a copy to avoid modifying the original DataFrame\n",
    "    df_cleaned = df.copy()\n",
    "    df_cleaned = df_cleaned.dropna(axis=0, how=how, subset=subset)\n",
    "    return df_cleaned"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c5f8379",
   "metadata": {},
   "source": [
    "# Working on..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9630a28",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Specify the directory path\n",
    "dir_path = \"../files/data/RAW\"\n",
    "\n",
    "# Get folder names\n",
    "master_folders = [f for f in os.listdir(dir_path) if os.path.isdir(os.path.join(dir_path, f))]\n",
    "\n",
    "print(master_folders)\n",
    "\n",
    "# Specify the directory path\n",
    "dir_path = \"../files/data/RAW/{}\".format(master_folders[4])\n",
    "\n",
    "# # Get folder names\n",
    "# folders = [f for f in os.listdir(dir_path) if os.path.isdir(os.path.join(dir_path, f))]\n",
    "\n",
    "# Get folder names that start with \"DOUBLE_\"\n",
    "folders = [f for f in os.listdir(dir_path) if os.path.isdir(os.path.join(dir_path, f)) and f.startswith('DOUBLE_')]\n",
    "\n",
    "print(folders)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33532659",
   "metadata": {},
   "source": [
    "# .DAT to .csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5fd349b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_file(file_info):\n",
    "    input_file_path, output_file_path = file_info\n",
    "    \n",
    "    rows = []\n",
    "    with open(input_file_path, 'r') as file:\n",
    "        lines = file.readlines()\n",
    "\n",
    "        W_R, a_c, a_t, r_t, b_t = None, None, None, None, None\n",
    "        c_index = 0\n",
    "\n",
    "        for i, line in enumerate(lines):\n",
    "            values = line.strip().split()\n",
    "            if not values:\n",
    "                continue\n",
    "            \n",
    "            if values[0] == \"Geometry:\":\n",
    "                crack = False\n",
    "                # Extract the coefficient multiplying R (which is W/R)\n",
    "                wr_match = re.search(r\"W=h=\\s*([\\d.]+)\\*R\", line)\n",
    "                if wr_match:\n",
    "                    W_R = float(wr_match.group(1))\n",
    "\n",
    "                # Extract b/t value\n",
    "                bt_match = re.search(r\"with\\s*b/t=([\\d.]+)\", line)\n",
    "                if bt_match:\n",
    "                    b_t = float(bt_match.group(1))\n",
    "\n",
    "            if values[0] == \"Scenario:\":\n",
    "                crack = False\n",
    "                c_index = values[1]\n",
    "\n",
    "            elif values[0] == \"ndom\" and (values[1] == \"a1/c1\" or values[1] == \"a2/c2\"):\n",
    "                crack = False\n",
    "                next_values = lines[i + 1].strip().split()\n",
    "                a_c, a_t, r_t = map(float, next_values[1:4])\n",
    "\n",
    "            elif values[0] == \"crack\":\n",
    "                crack = True\n",
    "\n",
    "            elif values[0].isdigit() and crack:\n",
    "                row = [c_index] + list(map(float, values[0])) + [W_R, a_c, a_t, r_t, b_t] + list(map(float, values[5:9]))\n",
    "                rows.append(row)\n",
    "\n",
    "    df = pd.DataFrame(rows, columns=['c_index', 'crack', 'W/R', 'a/c', 'a/t', 'r/t', 'b/t', 'phi', 'K-T', 'K-B', 'K-P'])\n",
    "    df.to_csv(output_file_path, index=False)\n",
    "\n",
    "master_folder = master_folders[4]\n",
    "tasks = []\n",
    "\n",
    "for folder in folders:\n",
    "    dir_path = os.path.join(\"../files/data/RAW\", master_folder, folder)\n",
    "    files = [f for f in os.listdir(dir_path) if os.path.isfile(os.path.join(dir_path, f))]\n",
    "\n",
    "    for file_name in files:\n",
    "        input_file_path = os.path.join(dir_path, file_name)\n",
    "        output_file_path = os.path.join(dir_path, f\"{file_name[:-6]}.csv\")\n",
    "        tasks.append((input_file_path, output_file_path))\n",
    "\n",
    "\n",
    "for i in tqdm(range(len(tasks))):\n",
    "    process_file(tasks[i])\n",
    "\n",
    "# with ThreadPoolExecutor() as executor:\n",
    "#     list(tqdm(executor.map(process_file, tasks), total=len(tasks), desc=\"Processing Files\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2e1cc22",
   "metadata": {},
   "source": [
    "# Cleaning CSV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84d0898d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assume drop_rows_with_nan is defined elsewhere and works correctly\n",
    "# Example placeholder:\n",
    "def drop_rows_with_nan(df):\n",
    "    \"\"\"Drops rows with any NaN values.\"\"\"\n",
    "    return df.dropna().copy() # Added .copy() to avoid SettingWithCopyWarning\n",
    "\n",
    "\n",
    "# --- Function to process a single c_index group ---\n",
    "# This function will handle the logic for one unique c_index\n",
    "def process_c_index_group(group_df):\n",
    "    \"\"\"\n",
    "    Processes a single DataFrame group corresponding to a unique c_index.\n",
    "    Performs crack splitting, parameter extraction, interpolation,\n",
    "    and constructs the 128x16 output block.\n",
    "\n",
    "    Returns the 128x16 numpy array block or None if processing fails.\n",
    "    \"\"\"\n",
    "    c_index = group_df['c_index'].iloc[0] # Get the c_index value for this group\n",
    "\n",
    "    # Split the group by crack type (assuming 'crack' is the column name)\n",
    "    try:\n",
    "        crack1_df = group_df[group_df['crack'] == 1]\n",
    "        crack2_df = group_df[group_df['crack'] == 2]\n",
    "\n",
    "        if crack1_df.empty or crack2_df.empty:\n",
    "             # This case should ideally be caught by the initial corrupt_indices check,\n",
    "             # but as a safeguard within the group processing.\n",
    "            print(f\"Warning: c_index {c_index} does not have data for both cracks 1 and 2 after grouping.\")\n",
    "            return None\n",
    "\n",
    "    except KeyError:\n",
    "        print(f\"Error: 'crack' column not found for c_index {c_index}.\")\n",
    "        return None\n",
    "    except Exception as e:\n",
    "        print(f\"Error splitting crack data for c_index {c_index}: {e}\")\n",
    "        return None\n",
    "\n",
    "\n",
    "    # --- Extract Unique Parameters ---\n",
    "    # Assuming these parameters are constant for a given c_index\n",
    "    # and match the column names implied by your original index access\n",
    "    try:\n",
    "        # Use .iloc[0] to get the first value, assuming they are constant within the group\n",
    "        wr = crack1_df['W/R'].iloc[0] # Original: data[:,1]\n",
    "        rt = crack1_df['r/t'].iloc[0] # Original: data[:,6]\n",
    "        bt = crack1_df['b/t'].iloc[0] # Original: data[:,7]\n",
    "\n",
    "        # These are crack-specific parameters\n",
    "        a1c1 = crack1_df['a/c'].iloc[0] # Original: data[:,2] (crack 1)\n",
    "        a1t = crack1_df['a/t'].iloc[0]  # Original: data[:,3] (crack 1)\n",
    "\n",
    "        a2c2 = crack2_df['a/c'].iloc[0] # Original: data[:,4] (crack 2)\n",
    "        a2t = crack2_df['a/t'].iloc[0]  # Original: data[:,5] (crack 2)\n",
    "\n",
    "    except (KeyError, IndexError) as e:\n",
    "         print(f\"Error extracting parameters for c_index {c_index}: {e}\")\n",
    "         return None\n",
    "    except Exception as e:\n",
    "        print(f\"Error extracting parameters for c_index {c_index}: {e}\")\n",
    "        return None\n",
    "\n",
    "    # --- Prepare for Interpolation ---\n",
    "    # Extract relevant columns and convert to numpy *once* per crack group\n",
    "    try:\n",
    "        # Assuming 'phi', 'K-T', 'K-B', 'K-P' are the last 4 columns as per original indexing logic\n",
    "        crack1_np = crack1_df[['phi', 'K-T', 'K-B', 'K-P']].to_numpy()\n",
    "        crack2_np = crack2_df[['phi', 'K-T', 'K-B', 'K-P']].to_numpy()\n",
    "    except KeyError:\n",
    "        print(f\"Error: Missing expected K or phi columns for c_index {c_index}.\")\n",
    "        return None\n",
    "    except Exception as e:\n",
    "        print(f\"Error converting crack data to numpy for c_index {c_index}: {e}\")\n",
    "        return None\n",
    "\n",
    "\n",
    "    # --- Interpolation Logic ---\n",
    "\n",
    "    # Determine the target phi grid (same for both cracks per c_index)\n",
    "    # Based on the original code's logic taking min/max phi from each crack data\n",
    "    try:\n",
    "        phi_vals1 = crack1_np[:, 0]\n",
    "        phi_vals2 = crack2_np[:, 0]\n",
    "\n",
    "        phi_min_combined = min(phi_vals1.min(), phi_vals2.min()) + 0.035\n",
    "        phi_max_combined = max(phi_vals1.max(), phi_vals2.max()) - 0.035\n",
    "\n",
    "        # Ensure valid range for linspace\n",
    "        if phi_min_combined >= phi_max_combined:\n",
    "            print(f\"Warning: Calculated phi_min >= phi_max for c_index {c_index}. Skipping interpolation.\")\n",
    "            return None\n",
    "\n",
    "        # Generate the target phi grid (128 points as per original code structure)\n",
    "        phi_grid_target = np.linspace(phi_min_combined, phi_max_combined, 132)[2:-2]\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Error calculating phi grid for c_index {c_index}: {e}\")\n",
    "        return None\n",
    "\n",
    "    # Helper function for interpolation (reduces code repetition)\n",
    "    def interpolate_crack_data(crack_np, phi_grid_target, phi_min_combined, phi_max_combined):\n",
    "        phi_vals = crack_np[:, 0]\n",
    "        k_t_vals = crack_np[:, 1]\n",
    "        k_b_vals = crack_np[:, 2]\n",
    "        k_p_vals = crack_np[:, 3]\n",
    "\n",
    "        # Apply phi range filtering using the combined min/max + offset\n",
    "        filtered_indices = (phi_vals >= phi_min_combined) & (phi_vals <= phi_max_combined)\n",
    "        phi_vals_filtered = phi_vals[filtered_indices]\n",
    "        k_t_vals_filtered = k_t_vals[filtered_indices]\n",
    "        k_b_vals_filtered = k_b_vals[filtered_indices]\n",
    "        k_p_vals_filtered = k_p_vals[filtered_indices]\n",
    "\n",
    "\n",
    "        # Need at least 2 data points for Pchip interpolation\n",
    "        if len(phi_vals_filtered) < 2:\n",
    "             print(f\"Warning: Not enough valid data points ({len(phi_vals_filtered)}) for interpolation.\")\n",
    "             return None, None, None\n",
    "\n",
    "        # Sort phi values for monotonic input required by PchipInterpolator\n",
    "        sort_indices = np.argsort(phi_vals_filtered)\n",
    "        phi_vals_sorted = phi_vals_filtered[sort_indices]\n",
    "\n",
    "        # Create monotonic indices (in case of identical phi values after filtering)\n",
    "        monotonic_indices = [0]\n",
    "        for i in range(1, len(phi_vals_sorted)):\n",
    "             if phi_vals_sorted[i] > phi_vals_sorted[monotonic_indices[-1]]:\n",
    "                 monotonic_indices.append(i)\n",
    "\n",
    "        if len(monotonic_indices) < 2:\n",
    "             print(f\"Warning: Not enough monotonic phi points ({len(monotonic_indices)}) for interpolation.\")\n",
    "             return None, None, None\n",
    "\n",
    "        # Perform Pchip interpolation for each K value\n",
    "        try:\n",
    "            interp_kt = PchipInterpolator(phi_vals_sorted[monotonic_indices], k_t_vals_filtered[sort_indices][monotonic_indices], extrapolate=False)\n",
    "            kt_interp = interp_kt(phi_grid_target)\n",
    "\n",
    "            interp_kb = PchipInterpolator(phi_vals_sorted[monotonic_indices], k_b_vals_filtered[sort_indices][monotonic_indices], extrapolate=False)\n",
    "            kb_interp = interp_kb(phi_grid_target)\n",
    "\n",
    "            interp_kp = PchipInterpolator(phi_vals_sorted[monotonic_indices], k_p_vals_filtered[sort_indices][monotonic_indices], extrapolate=False)\n",
    "            kp_interp = interp_kp(phi_grid_target)\n",
    "\n",
    "            return kt_interp, kb_interp, kp_interp\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"Error during Pchip interpolation: {e}\")\n",
    "            return None, None, None\n",
    "\n",
    "\n",
    "    # Perform interpolation for both cracks\n",
    "    kt1_interp, kb1_interp, kp1_interp = interpolate_crack_data(crack1_np, phi_grid_target, phi_min_combined, phi_max_combined)\n",
    "    kt2_interp, kb2_interp, kp2_interp = interpolate_crack_data(crack2_np, phi_grid_target, phi_min_combined, phi_max_combined)\n",
    "\n",
    "    # Check if interpolation was successful for both cracks\n",
    "    if any(k is None for k in [kt1_interp, kb1_interp, kp1_interp, kt2_interp, kb2_interp, kp2_interp]):\n",
    "        print(f\"Interpolation failed for c_index {c_index}. Skipping.\")\n",
    "        return None\n",
    "\n",
    "\n",
    "    # --- Construct the 128x16 Output Block ---\n",
    "    # Pre-allocate the block for this c_index\n",
    "    data_block = np.zeros((128, 16))\n",
    "\n",
    "    # Populate the block based on the desired column structure\n",
    "    data_block[:, 0] = c_index\n",
    "    data_block[:, 1] = wr\n",
    "    data_block[:, 2] = a1c1\n",
    "    data_block[:, 3] = a1t\n",
    "    data_block[:, 4] = a2c2 # a2/c2 from crack 2\n",
    "    data_block[:, 5] = a2t  # a2/t from crack 2\n",
    "    data_block[:, 6] = rt\n",
    "    data_block[:, 7] = bt\n",
    "    data_block[:, 8] = phi_grid_target # phi_1 is the target grid\n",
    "    data_block[:, 9] = phi_grid_target # phi_2 is the target grid\n",
    "    data_block[:, 10] = kt1_interp\n",
    "    data_block[:, 11] = kt2_interp\n",
    "    data_block[:, 12] = kb1_interp\n",
    "    data_block[:, 13] = kb2_interp\n",
    "    data_block[:, 14] = kp1_interp\n",
    "    data_block[:, 15] = kp2_interp\n",
    "\n",
    "    # Final check for NaNs resulting from interpolation outside valid range (extrapolate=False)\n",
    "    if np.isnan(data_block).any():\n",
    "        # print(f\"Warning: NaN values generated in final data block for c_index {c_index}. Skipping.\")\n",
    "        return None\n",
    "\n",
    "    return data_block\n",
    "\n",
    "\n",
    "# --- Main Processing Loop for Files ---\n",
    "\n",
    "dir_path = \"../files/data/TWIN/CS\"\n",
    "# Create the output directory if it doesn't exist\n",
    "output_dir = os.path.join(dir_path, \"CLEANED\")\n",
    "os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "\n",
    "csv_files = [f for f in os.listdir(dir_path) if f.endswith(\".csv\")]\n",
    "\n",
    "\n",
    "for csv_index in csv_files: # Continue processing from file 70\n",
    "    print(f\"Working on: {csv_index}\")\n",
    "    file_path = os.path.join(dir_path, csv_index)\n",
    "    output_path = os.path.join(output_dir, f\"{csv_index[:-4]}-CLEANED.csv\")\n",
    "\n",
    "    # Skip if the cleaned file already exists (optional, for resuming)\n",
    "    if os.path.exists(output_path):\n",
    "        print(f\"Cleaned file {output_path} already exists. Skipping.\")\n",
    "        continue\n",
    "\n",
    "    try:\n",
    "        # Read the CSV file\n",
    "        df = pd.read_csv(file_path)\n",
    "\n",
    "        # Drop rows with NaN values (assuming drop_rows_with_nan is efficient)\n",
    "        initial_rows = len(df)\n",
    "        df = drop_rows_with_nan(df)\n",
    "        if len(df) < initial_rows:\n",
    "            print(f\"Dropped {initial_rows - len(df)} rows with NaN in {csv_index}\")\n",
    "\n",
    "        if df.empty:\n",
    "             print(f\"No data left in {csv_index} after dropping NaNs. Skipping.\")\n",
    "             continue\n",
    "\n",
    "        # Check for corrupt indices where 'crack' count is not 2 per 'c_index'\n",
    "        # Assuming 'crack' is the correct column name here based on your code\n",
    "        if 'c_index' not in df.columns or 'crack' not in df.columns:\n",
    "             print(f\"Error: Missing 'c_index' or 'crack' column in {csv_index}. Skipping.\")\n",
    "             continue\n",
    "\n",
    "        unique_counts = df.groupby('c_index')['crack'].nunique()\n",
    "        corrupt_indices = unique_counts[unique_counts != 2].index\n",
    "\n",
    "        # Filter out rows belonging to corrupt c_index values\n",
    "        df = df[~df['c_index'].isin(corrupt_indices)].copy() # Use .copy() to avoid SettingWithCopyWarning\n",
    "\n",
    "        unique_index_list = df['c_index'].unique().tolist()\n",
    "        num_valid_indices = len(unique_index_list)\n",
    "\n",
    "        if num_valid_indices == 0:\n",
    "             print(f\"No valid c_index entries left in {csv_index} after cleaning. Skipping.\")\n",
    "             continue\n",
    "\n",
    "        # --- Process Groups and Collect Results ---\n",
    "        processed_blocks = []\n",
    "\n",
    "        # Group by c_index and iterate through groups\n",
    "        grouped = df.groupby('c_index')\n",
    "\n",
    "        # Use tqdm to show progress over the groups within the file\n",
    "        for c_index, group_df in tqdm(grouped, desc=f\"Processing {csv_index}\", unit=\"group\"):\n",
    "            # Process each group using the dedicated function\n",
    "            data_block = process_c_index_group(group_df) # Pass the group DataFrame\n",
    "\n",
    "            if data_block is not None:\n",
    "                processed_blocks.append(data_block)\n",
    "\n",
    "        # --- Combine and Save ---\n",
    "        if not processed_blocks:\n",
    "             print(f\"No valid processed blocks generated for {csv_index}. Skipping save.\")\n",
    "             continue\n",
    "\n",
    "        # Concatenate all collected blocks into the final array\n",
    "        d_final = np.concatenate(processed_blocks, axis=0)\n",
    "\n",
    "        # Convert the final numpy array to a DataFrame\n",
    "        # Ensure column names match the structure of d_final\n",
    "        output_df = pd.DataFrame(d_final, columns=[\n",
    "            \"c_index\", \"W/R\", \"a1/c1\", \"a1/t\", \"a2/c2\", \"a2/t\",\n",
    "            \"r/t\", \"b/t\", \"phi_1\", \"phi_2\", \"K1-T\", \"K2-T\",\n",
    "            \"K1-B\", \"K2-B\", \"K1-P\", \"K2-P\"\n",
    "        ])\n",
    "\n",
    "        # Save the cleaned and processed DataFrame to a new CSV file\n",
    "        output_df.to_csv(output_path, index=False) # index=False prevents writing the DataFrame index\n",
    "        print(f\"Successfully saved cleaned data for {csv_index} to {output_path}\")\n",
    "\n",
    "    except FileNotFoundError:\n",
    "        print(f\"Error: File not found at {file_path}. Skipping.\")\n",
    "        continue\n",
    "    except Exception as e:\n",
    "        print(f\"An unexpected error occurred while processing {csv_index}: {e}\")\n",
    "        # Optional: print traceback for debugging\n",
    "        # import traceback\n",
    "        # traceback.print_exc()\n",
    "        continue # Continue to the next file\n",
    "\n",
    "\n",
    "print(\"Processing complete.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2a05116",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Specify the directory path\n",
    "dir_path = \"../files/data/TWIN/CS\"\n",
    "\n",
    "# Get all .csv files\n",
    "csv_files = [f for f in os.listdir(dir_path) if f.endswith(\".csv\")]\n",
    "\n",
    "\n",
    "for csv_index in csv_files[70:]:\n",
    "    print(\"Working on: \", csv_index)\n",
    "    df = pd.read_csv(\"../files/data/TWIN/CS/{}\".format(csv_index))\n",
    "    df = drop_rows_with_nan(df)\n",
    "\n",
    "    unique_index = df['c_index'].unique()\n",
    "    \n",
    "    column_to_check = \"crack\" # Or replace with the actual column name string, e.g., 'crack_type'\n",
    "\n",
    "    # --- Optimized Logic ---\n",
    "\n",
    "    # 1. Group by 'c_index' and count unique values in the target column for each group\n",
    "    unique_counts = df.groupby('c_index')[column_to_check].nunique()\n",
    "\n",
    "    # 2. Filter the result to find indices where the unique count is NOT 2\n",
    "    corrupt_indices = unique_counts[unique_counts != 2].index\n",
    "\n",
    "    # 3. Convert the resulting index object to a list (if you need a list)\n",
    "    corrupt_list = corrupt_indices.tolist()\n",
    "\n",
    "    df = df[~df['c_index'].isin(corrupt_list)]\n",
    "\n",
    "    unique_index = df['c_index'].unique()\n",
    "\n",
    "    d_final = np.zeros((len(unique_index)*128, 16))\n",
    "\n",
    "    ui = 0\n",
    "    for unq_index in tqdm(unique_index):\n",
    "        data = np.zeros((128, 16))\n",
    "        filtered_df = df[df[\"c_index\"] == unq_index]\n",
    "\n",
    "        cracks = np.unique(filtered_df.to_numpy()[:,1])\n",
    "\n",
    "        data[:,0] = unq_index\n",
    "        for crack in cracks:\n",
    "            filtered_df_ = filtered_df[filtered_df[\"crack\"] == crack]\n",
    "\n",
    "            filtered_d_ = filtered_df_.to_numpy()\n",
    "\n",
    "            W_R = np.unique(filtered_d_[:,2])\n",
    "            a_c = np.unique(filtered_d_[:,3])\n",
    "            a_t = np.unique(filtered_d_[:,4])\n",
    "            r_t = np.unique(filtered_d_[:,5])\n",
    "            b_t = np.unique(filtered_d_[:,6])\n",
    "\n",
    "            assert len(W_R) == 1, W_R\n",
    "            assert len(a_c) == 1, a_c\n",
    "            assert len(a_t) == 1, a_t\n",
    "            assert len(r_t) == 1, r_t\n",
    "            assert len(b_t) == 1, b_t\n",
    "\n",
    "            phi_vals = filtered_d_[:,-4]\n",
    "            phi_min = phi_vals.min() + 0.035\n",
    "            phi_max = phi_vals.max() - 0.035\n",
    "            # Filter indices where phi values lie within [phi_min, phi_max]\n",
    "            filtered_indices = (phi_vals >= phi_min) & (phi_vals <= phi_max)\n",
    "            phi_vals = phi_vals[filtered_indices]\n",
    "            assert len(phi_vals) > 16, len(phi_vals)\n",
    "            phi_idxes = np.argsort(phi_vals)\n",
    "            \n",
    "            monotonic_phi_idxes = [phi_idxes[0]]\n",
    "            prev_phi = phi_vals[phi_idxes[0]]\n",
    "            for index in phi_idxes[1:]:\n",
    "                now_phi = phi_vals[index]\n",
    "                if now_phi > prev_phi:\n",
    "                    monotonic_phi_idxes.append(index)\n",
    "                    prev_phi = phi_vals[index]\n",
    "\n",
    "            phi_regular_128 = np.linspace(phi_min, phi_max, 132)\n",
    "\n",
    "            # Tension\n",
    "            K_vals = filtered_d_[:,-3]\n",
    "            K_vals = K_vals[filtered_indices]\n",
    "            \n",
    "            interp_func = PchipInterpolator(phi_vals[monotonic_phi_idxes], K_vals[monotonic_phi_idxes], extrapolate=False)\n",
    "            KT_regular_128 = interp_func(phi_regular_128)\n",
    "\n",
    "            # Bending\n",
    "            K_vals = filtered_d_[:,-2]\n",
    "            K_vals = K_vals[filtered_indices]\n",
    "            \n",
    "            interp_func = PchipInterpolator(phi_vals[monotonic_phi_idxes], K_vals[monotonic_phi_idxes], extrapolate=False)\n",
    "            KB_regular_128 = interp_func(phi_regular_128)\n",
    "\n",
    "            # Pin\n",
    "            K_vals = filtered_d_[:,-1]\n",
    "            K_vals = K_vals[filtered_indices]\n",
    "            \n",
    "            interp_func = PchipInterpolator(phi_vals[monotonic_phi_idxes], K_vals[monotonic_phi_idxes], extrapolate=False)\n",
    "            KP_regular_128 = interp_func(phi_regular_128)\n",
    "\n",
    "            if crack == 1:\n",
    "                data[:,1] = W_R[0]\n",
    "                data[:,2] = a_c[0]\n",
    "                data[:,3] = a_t[0]\n",
    "                data[:,6] = r_t[0]\n",
    "                data[:,7] = b_t[0]\n",
    "                data[:,8] = phi_regular_128[2:-2]\n",
    "                data[:,10] = KT_regular_128[2:-2]\n",
    "                data[:,12] = KB_regular_128[2:-2]\n",
    "                data[:,14] = KP_regular_128[2:-2]\n",
    "            \n",
    "            elif crack == 2:\n",
    "                data[:,1] = W_R[0]\n",
    "                data[:,4] = a_c[0]\n",
    "                data[:,5] = a_t[0]\n",
    "                data[:,6] = r_t[0]\n",
    "                data[:,7] = b_t[0]\n",
    "                data[:,9] = phi_regular_128[2:-2]\n",
    "                data[:,11] = KT_regular_128[2:-2]\n",
    "                data[:,13] = KB_regular_128[2:-2]\n",
    "                data[:,15] = KP_regular_128[2:-2]\n",
    "            \n",
    "            else:\n",
    "                print(\"Something is wrong with crack\")\n",
    "\n",
    "        if np.isnan(data).any():\n",
    "            continue\n",
    "        else:\n",
    "            d_final[ui*128:ui*128+128] = data\n",
    "            ui += 1\n",
    "\n",
    "    d_final = d_final[~np.all(d_final == 0, axis=1)]\n",
    "\n",
    "    df = pd.DataFrame(d_final, columns=[\"c_index\", \"W/R\", \"a1/c1\", \"a1/t\", \"a2/c2\", \"a2/t\", \n",
    "                                    \"r/t\", \"b/t\", \"phi_1\", \"phi_2\", \"K1-T\", \"K2-T\", \n",
    "                                    \"K1-B\", \"K2-B\", \"K1-P\", \"K2-P\"])\n",
    "\n",
    "    df.to_csv(\"../files/data/TWIN/CS/CLEANED/{}-CLEANED.csv\".format(csv_index[:-4]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96d6e906",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(0)\n",
    "fig, axs = plt.subplots(2, 5, figsize=(30,8))\n",
    "unique_index = np.unique(d_final[:,0])\n",
    "print(len(unique_index))\n",
    "for i in range(2):\n",
    "    for j in range(5):\n",
    "        idx = np.random.randint(0, len(unique_index))\n",
    "        data = d_final[idx*128:idx*128+128]\n",
    "        W_R = np.unique(data[:,1])\n",
    "        a1_c1 = np.unique(data[:,2])\n",
    "        a1_t = np.unique(data[:,3])\n",
    "        a2_c2 = np.unique(data[:,4])\n",
    "        a2_t = np.unique(data[:,5])\n",
    "        r_t = np.unique(data[:,6])\n",
    "\n",
    "        assert len(W_R) == 1, W_R\n",
    "        assert len(a1_c1) == 1\n",
    "        assert len(a1_t) == 1\n",
    "        assert len(a2_c2) == 1\n",
    "        assert len(a2_t) == 1\n",
    "        assert len(r_t) == 1\n",
    "\n",
    "        axs[i,j].scatter(data[:,8], data[:,10], label=\"K1-T\", color='purple', s=10)\n",
    "        axs[i,j].plot(data[:,8], data[:,10], color='purple', linestyle=\":\")\n",
    "\n",
    "        axs[i,j].scatter(data[:,9], data[:,11], label=\"K2-T\", color='green', s=10)\n",
    "        axs[i,j].plot(data[:,9], data[:,11], color='green', linestyle=\":\")\n",
    "\n",
    "        axs[i,j].set_title(\"W/R:{} a1/c1:{} a1/t:{} a2/c2:{} a2/t:{} r/t: {}\".format(W_R[0],\n",
    "                                                                              a1_c1[0],\n",
    "                                                                              a1_t[0],\n",
    "                                                                              a2_c2[0],\n",
    "                                                                              a2_t[0],\n",
    "                                                                              r_t[0]))\n",
    "        \n",
    "        if i == 0 and j == 0:\n",
    "            axs[i,j].legend()\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75ae70e7",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
